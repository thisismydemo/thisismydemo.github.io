---
title: "Beyond the Cloud: Hardware Considerations - Part III"
description: Can you reuse existing SANs and mixed servers, or do you need validated nodes? Build your cloud on your terms with the right hardware strategy.
date: 2025-06-26T12:52:40.687Z
preview: /img/rethinkvmware/hardware-part3-banner.png
draft: true
tags:
    - Hardware
    - Storage
    - SAN
    - Validated Nodes
    - Infrastructure
    - Hyper-V
    - Azure Local
categories:
    - Infrastructure
    - Hardware
    - Virtualization
thumbnail: /img/rethinkvmware/hardware-part3-banner.png
lead: Can you reuse existing SANs and mixed servers, or do you need validated nodes? Build your cloud on your terms.
slug: beyond-cloud-hardware-considerations-part-iii
lastmod: 2025-06-26T18:01:56.620Z
---

# Hardware Considerations: Build Your Cloud on Your Terms

**Series Recap:** In **Part 1** of this series, we examined the total cost of ownership (TCO) implications of different post-VMware paths, comparing capital expenditure vs. subscription models across on-premises Hyper-V, Azure Local (formerly Azure Stack HCI), and Azure VMware Solution (AVS). In **Part 2**, we dove into licensing – analyzing how VMware vSphere licensing stacks up against Microsoft's offerings (Windows Server and Azure Local) in 2025, and what those licensing differences mean for choosing a virtualization platform. These earlier posts highlighted that organizations leaving VMware have viable Microsoft-based alternatives that can reduce costs and simplify licensing. Now, in **Part 3**, we turn to the **infrastructure** question: **What are your hardware options when "rethinking virtualization" away from VMware?** Can you reuse your **existing servers and storage**, or are you forced into buying **new, validated hardware nodes**? How do Microsoft's two on-premises solutions – **Windows Server Hyper-V with Failover Clustering (WSFC)** and **Azure Local** – compare in terms of hardware requirements? We'll explore scenarios for customers looking to leave VMware, whether they're **not ready for a hardware refresh** or **planning a refresh alongside the migration**, and we'll also briefly touch on upcoming **VMware Cloud Foundation 9.0** hardware needs.

## Series Navigation

- **Introduction**: [Beyond the Cloud: Rethinking Virtualization Post-VMware](https://thisismydemo.cloud/post/rethinking-virtualization-post-vmware/)
- **Part I**: [Beyond the Cloud: CapEx vs Subscription TCO Analysis](https://thisismydemo.cloud/post/capex-subscription-tco-modeling-hyper-azure-local-avs/)
- **Part II**: [Beyond the Cloud: 2025 Virtualization Licensing Guide](https://thisismydemo.cloud/post/choosing-your-virtualization-platform-2025-licensing-analysis/)
- **Part III**: Hardware Considerations - Build Your Cloud on Your Terms *(This Post)*

## Table of Contents

- [Hardware Considerations: Build Your Cloud on Your Terms](#hardware-considerations-build-your-cloud-on-your-terms)
  - [Series Navigation](#series-navigation)
  - [Table of Contents](#table-of-contents)
  - [Reuse vs. Refresh: Two Paths for Your Hardware](#reuse-vs-refresh-two-paths-for-your-hardware)
  - [Windows Server on Existing Infrastructure: SAN or S2D?](#windows-server-on-existing-infrastructure-san-or-s2d)
  - [Azure Local: New Hardware and Validated Nodes](#azure-local-new-hardware-and-validated-nodes)
  - [Note on VMware Cloud Foundation 9.0 Hardware Requirements](#note-on-vmware-cloud-foundation-90-hardware-requirements)
  - [Side-by-Side Comparison](#side-by-side-comparison)
  - [Conclusion: Choose the Path that Fits Your Needs](#conclusion-choose-the-path-that-fits-your-needs)

## Reuse vs. Refresh: Two Paths for Your Hardware

Every VMware-using organization has a unique timeline for hardware. Some need to **migrate off VMware soon** (e.g., before a costly license renewal) while their servers and SAN storage still have plenty of life. Others might see VMware’s end as an opportunity to **refresh hardware** altogether. This distinction is critical when choosing a path forward:

* **Path 1: Reuse Existing Hardware.** If your current servers and storage ran VMware vSphere reliably, you might prefer to repurpose them for the new solution. This path favors a platform that is **flexible with hardware compatibility**. Microsoft’s traditional Windows Server + Hyper-V is very accommodating here – it can run on a wide range of commodity servers and supports classic external SAN storage for clusters. In fact, Windows Server 2025 (with Hyper-V and Failover Clustering) offers much greater flexibility to **leverage existing gear** than Azure Local does. If your goal is to **maximize use of existing hardware**, the Windows Server route is often the best choice. It allows clustering with either shared storage (e.g., an iSCSI or FC SAN or NAS) or with internal disks via Storage Spaces Direct – giving you options to integrate with whatever infrastructure you already have.

* **Path 2: Refresh with New Hardware.** Some organizations align their hardware refresh cycle with moving off VMware. Microsoft’s **Azure Local** (formerly Azure Stack HCI) (now under the “Azure Local” branding) is positioned as a modern hyper-converged solution, but it **almost always requires new hardware** to meet its certified solution requirements. Azure Local runs on specific **validated node** configurations – typically turnkey integrated systems from OEM partners (Dell, HPE, Lenovo, etc.) – rather than just any random server. In fact, Microsoft notes that you can only repurpose existing servers for Azure Local if they **exactly match** a current validated node profile in the Azure Local Catalog. In practice, unless your older servers were originally purchased as a supported HCI solution, this is rarely the case. **In 99.9% of cases, adopting Azure Local means investing in new, certified hardware**. The Azure Local program emphasizes using approved hardware to ensure stability – following the **validated hardware path** helps avoid “ghost chasing” of obscure firmware issues under load. OEMs provide jointly supported solutions, and Microsoft “highly recommends” integrated systems that come pre-built and pre-tested for Azure Local. The result is a rock-solid platform, but at the cost of **being locked into new hardware purchases** in the near term.

In short, **Windows Server Hyper-V (WSFC)** offers a *“build your cloud on your terms”* approach – you can likely reuse your existing mix of servers and storage – whereas **Azure Local** usually mandates a clean slate with new, homogenized hardware. Next, we’ll compare these options in detail: using Windows Server on your current infrastructure (either with your SAN or with Storage Spaces Direct) versus rolling out Azure Local on vendor-validated nodes.

## Windows Server on Existing Infrastructure: SAN or S2D?

One of the strengths of the Windows Server Failover Clustering approach is its **hardware flexibility**. You have two architecture choices for storage when using WSFC + Hyper-V:

* **Traditional SAN-based Cluster (Reuse Your SAN):** If you have an existing Fibre Channel or iSCSI SAN that’s still performant and supported, you can continue to leverage it with a Windows Server Hyper-V cluster. Microsoft fully supports “traditional infrastructure” setups where VMs on a Windows cluster use an external shared SAN/NAS for storage. In this model, each Hyper-V host connects to the SAN LUNs (or shares) and those LUNs are configured as Cluster Shared Volumes (CSVs) accessible by all nodes. This essentially replicates the VMware + SAN design with a different hypervisor. The benefit is obvious: you **protect your prior investments** in storage. For example, if you recently bought an all-flash SAN that has years of life left, moving to WSFC/Hyper-V lets you **swap the hypervisor but keep the storage** – an incremental migration that minimizes new spend. Administrators who are comfortable with their SAN vendor’s tools and best practices can stick with what they know, just integrating it with Windows instead of ESXi. Microsoft explicitly acknowledges that Windows Server remains ideal for these scenarios (multiple hosts clustered with a shared disk array), whereas Azure Local **does not support external SAN storage** at all. In fact, Azure Local's reliance on Storage Spaces Direct means it **cannot use** a fibre channel or iSCSI SAN for VM storage – there's no option for multi-path shared LUNs in Azure Local's design. Windows Server, on the other hand, can certainly use SAN/NAS storage for clustering, including using multi-path IO (MPIO) with identical HBAs across nodes for robust SAN connectivity. If you have a reliable SAN and processes built around it, **WSFC lets you keep that architecture** while moving away from VMware. This path is especially attractive for organizations facing a VMware license deadline but whose hardware (servers *and* SAN) is not due for refresh – you can transition to Hyper-V on your existing hosts, connect them to the existing SAN, and avoid a costly all-at-once hardware purchase.

* **Storage Spaces Direct (Hyper-Converged) Cluster:** For organizations that don’t have an external SAN, or who want to move toward a hyper-converged model using **local disks**, Windows Server Datacenter edition offers **Storage Spaces Direct (S2D)** as an option. S2D allows a WSFC cluster to pool internal drives on each host into a distributed, highly-available storage array – conceptually similar to VMware vSAN. The appeal of S2D is that you can eliminate the external SAN and use commodity disks (SSD/NVMe) inside the Hyper-V hosts to get SAN-like performance. If your existing VMware hosts have ample drive bays (or you’re willing to add some storage to them), you could repurpose those servers into an S2D cluster. This route might require some updates – e.g., adding SSDs or NVMe disks, and ensuring you have a high-bandwidth network between nodes (10 GbE or higher is recommended, with RDMA for best results). It’s also ideal to use identical or very similar servers for S2D clusters, although it’s not a strict requirement; Microsoft recommends matching hardware for consistency, and cluster validation will flag major discrepancies, but it doesn’t require the nodes to be identical models. In practice, successful S2D deployments typically use the same make/model servers with consistent drives and NICs for predictable performance. Assuming your hardware meets the requirements, S2D can be a cost-effective way to get an all-software-defined infrastructure. **Cost & management:** Because S2D is built into Windows Server, you avoid the expense of an external SAN array. In fact, a Hyper-V cluster with S2D is often **much cheaper than buying a new high-end SAN** from a vendor. You’re using industry-standard drives and controllers, and the “SAN intelligence” is provided by Windows Server itself. Administration can also be simpler: storage management is unified with the OS (PowerShell, Windows Admin Center, etc.) instead of dealing with separate SAN management GUIs. On the flip side, be mindful of the cluster size and resiliency: a **2-node S2D cluster** is possible (with witness) but uses simple two-way mirroring (50% capacity efficiency), whereas 3+ nodes enable more efficient resiliency (like three-way mirror or parity) and higher uptime during maintenance. If reusing existing hardware for S2D, ensure the controllers are in HBA mode (pass-through) and not RAID, and that any older NICs are capable of the throughput needed – a 1 Gbps network is technically supported but will be a bottleneck; 10 Gbps+ and RDMA is strongly recommended for HCI scenarios. In summary, **Windows Server S2D** gives you a path to hyper-converged infrastructure without forcing new hardware purchases – you can *gradually* modernize by, say, adding some SSDs and 10/25 Gb NICs to your existing servers rather than replacing everything outright. Many organizations find that attractive when budgets are tight. And if you *do* decide to buy new servers for a Hyper-V cluster, you can do so on your own terms (any hardware on the Windows Server HCL that passes cluster validation is supported) rather than being limited to HCI-certified nodes.

**Mixed Server Environments:** It’s worth noting that Windows Server clusters can tolerate a degree of hardware heterogeneity. It’s generally best practice to use homogeneous nodes in a cluster, but Microsoft support’s stance is simply that all components must be certified for Windows Server and the cluster must pass validation. That means you could have, for example, two slightly different server models or memory sizes in one cluster, and as long as they function well together (and you might enable CPU compatibility mode for live migrations across different CPU generations), it’s a supported config. Many Hyper-V deployments over the years have mixed hardware during transitions (adding newer nodes to an old cluster during upgrades, etc.). **Azure Local, by contrast, requires uniformity** – the official requirement is that *“all servers must be the same manufacturer and model”* in an HCI cluster. The HCI operating system expects symmetric hardware for storage pooling and uses an image-based lifecycle approach, so mixing server types is not part of its design. In short, WSFC gives you more wiggle room to **mix and match hardware to a degree** (helpful when repurposing existing gear), whereas Azure Local expects essentially identical nodes (typically delivered as a batch from an OEM) for each cluster.

## Azure Local: New Hardware and Validated Nodes

Azure Local (formerly Azure Stack HCI) is Microsoft's premier hybrid cloud-integrated HCI platform – but it comes with strict hardware prescriptions. If you're considering this route as your VMware replacement, here are the key hardware considerations:

* **Validated Hardware Only:** Azure Local is **sold as a solution**, not just software. Microsoft works with hardware partners to provide a catalog of validated node configurations and integrated systems. In fact, you cannot simply install Azure Local on any random server and call it a supported production deployment – it needs to align with a supported combination of components (CPU, drives, NICs, firmware, etc.). As noted earlier, existing servers can only be used if they **exactly match a current validated node profile**. For the vast majority of environments, this means **buying new nodes** that are listed in the Azure Local Catalog. These are often sold in pre-configured clusters of 2, 4, or more nodes, with all identical hardware. This ensures that the cluster will pass Microsoft's HCI validation and that you have a single throat to choke for support (often the OEM will handle first-line support for the hardware, with Microsoft covering the software). The emphasis on validated hardware is to ensure reliability – Microsoft and OEMs run extensive stress tests on these configurations. Deploying on something off-list risks hitting firmware or driver issues under load, which is why *"following the validated hardware path"* is strongly urged. In the HCI world, an misbehaving component can bring big headaches, so Microsoft's stance (echoed by partners like Dell) is to stick to known-good builds to **avoid playing whack-a-mole with obscure hardware issues**.

* **Hyper-Converged Storage (No SAN):** Azure Local's architecture is inherently hyper-converged – it **requires Storage Spaces Direct** using local disks in each node. Unlike a Windows Server cluster, **you cannot use an external SAN** with Azure Local for your VM storage. The system will not even allow adding a shared LUN to the cluster; all storage must be direct-attached to the hosts (or in a single-node scenario, attached JBOD, but not shared between nodes). This means that if you have an existing SAN, Azure Local provides no way to incorporate it – that storage would become effectively sidelined (perhaps used for backups or secondary storage, but not for the HCI cluster's primary workloads). Azure Local's philosophy is to instead use internal NVMe, SSD, and possibly HDDs to create a software-defined pool. This delivers excellent performance (especially with all-flash NVMe configurations) and simplifies management to one layer. But it reinforces that moving to Azure Local is a **full infrastructure replacement**: new servers *with their own internal storage* handle everything. It's an all-in-one approach.

* **Network Requirements:** Given the heavy reliance on east-west traffic for S2D replication, Azure Local has higher network demands. Realistically, 10 GbE is the minimum, and 25–100 Gb networks with RDMA are recommended for all-flash scenarios. Many older VMware deployments on 1 GbE won't meet Azure Local's performance needs. Thus, adopting HCI might involve upgrading network switches and NICs along with the servers. (Windows Server clusters with a SAN can sometimes get by with lower network throughput since the heavy lifting is offloaded to the SAN fabric; HCI pushes that onto the cluster network.)

* **Lifecycle and Updates:** Azure Local is delivered as an OS with frequent feature updates (semi-annual releases). OEM integrated systems often include streamlined update tools (e.g., firmware + driver + OS updates in one workflow). This is a benefit of buying into the validated model: you get a cohesive experience for patching. However, it also underscores that **older hardware may not be able to catch up** – each new HCI release might drop support for out-of-date devices. (For instance, if an older RAID controller or NIC isn't re-certified by the OEM for the latest HCI version, you might not be able to upgrade that cluster without hardware changes. This is similar to what VMware is doing with vSphere – more on that shortly.)

In summary, Azure Local is a powerful solution if you're aiming for a **modern, cloud-connected datacenter** and are willing (and able) to invest in **new infrastructure that meets the spec**. It shines in scenarios like distributed branch office deployments, edge locations, or a fresh private cloud build that needs Azure integration. But for customers coming from VMware who have a lot of sunk cost in their current hardware, HCI's hardware requirements can be a tough pill – it's essentially a **forklift upgrade** in many cases. This is exactly why we've been highlighting the Windows Server alternative in this series: it lets you incrementally adopt new technology on your timeline. Microsoft's marketing might paint Azure Local as the only forward path, but as we see, **WSFC/Hyper-V offers a more hardware-agnostic approach** that can be just as viable, especially during transition periods.

## Note on VMware Cloud Foundation 9.0 Hardware Requirements

Before we conclude, a brief aside for those weighing staying with VMware versus switching: VMware’s next-gen platform (vSphere 9.0, packaged in **VMware Cloud Foundation (VCF) 9.0**) also brings hardware considerations that might influence your strategy. VMware is raising the bar on supported hardware in vSphere 9. For example, a number of older server I/O devices (storage controllers, network adapters, etc.) are being deprecated in **ESXi 9.0**. VMware’s compatibility guides indicate some devices that were supported in vSphere 7 or 8 will not be supported going forward unless vendors provide updated drivers. In fact, VMware has warned that if you attempt to upgrade to ESXi 9.0 on a host that contains an **end-of-life (unsupported) device**, the upgrade may fail or, even if it succeeds, you could **lose access to storage or networking on that host**. In other words, running vSphere 9 will likely require weeding out any legacy HBAs or NICs that aren’t on the approved list. VMware’s guidance is clear: **replace deprecated hardware before upgrading**. This means that even if you wanted to stick with VMware on the same physical servers, you might have to invest in hardware upgrades (new NICs, RAID cards, etc.) to stay on a supported configuration. So either path – moving to Microsoft or staying with VMware’s latest – points to hardware updates sooner or later. The advantage of the Microsoft path with Windows Server is that you could potentially run on your existing hardware *for a while longer* (since Windows is generally tolerant as long as drivers exist), buying you time until a planned hardware refresh. In contrast, pushing into VMware’s newest stack might force an immediate hardware refresh anyway. It’s an important factor to weigh: leaving VMware could actually extend the useful life of your current servers if you go with a more flexible platform.

## Side-by-Side Comparison

To crystallize the differences, here's a side-by-side look at hardware considerations for **Windows Server (WSFC) vs. Azure Local**, along with a note on VMware's requirements:

| **Option**                                                            | **Hardware Flexibility**                                                                                                                                                                                                             | **Storage Architecture**                                                                                                                                                                                                                              | **Hardware Requirements**                                                                                                                                                                                                                                                            | **Ideal Use Case**                                                                                                                                                                                                                                                    |
| --------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Windows Server 2022/2025** <br> *Hyper-V + Failover Cluster* (WSFC) | **High:** Can reuse existing servers (even mixed models, if compatible). Supports broad range of certified hardware.                                                                                                                 | Flexible: Use **external SAN/NAS** or **Storage Spaces Direct** (local disks) – your choice. Can integrate with existing Fibre Channel/iSCSI storage (Azure Local cannot).                                                                              | Standard Windows Server HCL certification for NICs, HBAs, etc. **Cluster validation** must pass on chosen config. Recommended to use similar servers, but not strictly required.                                                                                                     | Best when you want to **maximize existing investments** – e.g., keep using a current SAN or defer new hardware costs. Also suitable for mixed environments and gradual upgrades.                                                                                       |
| **Azure Local** <br> *Hyper-Converged HCI OS*       | **Low:** Generally **requires new, vendor-validated nodes**. Existing hardware only usable if it exactly matches a supported config. Uniform servers (same model) required in cluster.                                               | **Hyper-converged only:** Uses **Storage Spaces Direct** with internal drives. **No support for external SAN** or shared SAS enclosures. All nodes use local SSD/NVMe for storage, replicated across cluster.                                         | Must purchase from **Azure Local Catalog** of solutions. Typically 2-16 nodes, all-flash or hybrid drives, with 10–100 GbE RDMA networking. **Strict validation** – non-certified components (RAID cards, older NICs) are not allowed. Ongoing Azure subscription per core for the OS. | Best when undertaking a **full infrastructure refresh** and wanting a cloud-integrated solution. Ideal if you plan to modernize to HCI and can invest in new hardware that aligns with Azure’s hybrid vision.                                                         |
| **VMware vSphere/VCF 9.0** <br> *(for comparison)*                    | **Medium:** vSphere supports a wide hardware range *in general*, but **upgrading to 9.0 will drop support for many older devices** (e.g., some HBAs, NICs). Likely requires hardware refresh or component upgrades for older servers. | Flexible: Supports traditional SAN/NAS or vSAN (HCI) if you have appropriate licenses. vSAN (like S2D) requires local disks and usually homogeneous nodes, whereas using a SAN is still an option in vSphere (one reason some may stick with VMware). | **Requires devices on VMware’s compatibility list.** ESXi 9.0 deprecates certain older devices – they’ll operate in 8.x but must be replaced before 9.0. Otherwise, standard x86 server requirements (similar to WSFC in CPU/RAM needs).                                             | Best if you intend to **stay with VMware** and are willing to refresh hardware as needed. (If you have a significant investment in VMware-specific tooling and skills, you may prefer this route, but watch out for Broadcom/VMware’s licensing and support changes.) |

*(Note: AVS – Azure VMware Solution – is not covered in detail here since it’s essentially outsourcing VMware onto Azure’s hardware. That involves a different calculus: you avoid buying hardware entirely, but you pay cloud rental fees and must fit into Azure’s instance constraints. In this post, we focus on on-premises alternatives where you control the hardware.)*

## Conclusion: Choose the Path that Fits Your Needs

As you plan your post-VMware journey, the hardware dimension is as important as cost and licensing. Microsoft’s two on-premises offerings present two different philosophies:

* **Windows Server + Hyper-V (WSFC)** empowers you to **build your cloud on *your* terms** – you can start with what you have and evolve gradually. It’s a proven, enterprise-grade platform that, despite getting less hype these days, continues to run mission-critical workloads worldwide. This path is about **pragmatism**: if you’re staring down a VMware exit but don’t have the budget (or desire) to rip-and-replace your infrastructure, WSFC lets you transition to a supported, modern virtualization platform with minimal hardware changes. You can always perform a phased hardware refresh later, on your schedule (for instance, replace aging servers with newer ones for your Hyper-V cluster over time, without changing the software stack).

* **Azure Local** offers a more **prescriptive, cloud-connected** on-prem solution – great for organizations that want the latest tech and tight Azure integration, and are ready to invest in that upfront. It brings the benefits of hyper-converged design and Azure Arc management, but it’s less forgiving of legacy gear. This path makes sense if you’ve decided to modernize your data center and have budget allocated for new HCI nodes (or if your existing hardware is truly at end-of-life and *needs* replacement anyway). It’s the route Microsoft’s sales teams will often promote for “transforming” your on-premises environment.

Ultimately, the choice comes down to your **business requirements and timelines**. If avoiding a big capital spend and extending equipment lifespan is a priority, then leveraging Windows Server 2025 with Hyper-V on your existing hardware can be a smart move – it provides a **fully supported platform** without forcing you into new purchases. If, on the other hand, you desire a cutting-edge, cloud-hybrid infrastructure and are prepared to standardize on new hardware, Azure Local can deliver that experience (just go in with eyes open about the ongoing subscription and the one-time hardware costs).

Remember that there’s no one-size-fits-all “better” option. It’s about aligning the solution to your organization’s needs. The good news is that **Microsoft gives you both options** – just don’t let the marketing convince you that you *must* go the Azure Local route. As this series argues, **Windows Server + Hyper-V remains a rock-solid alternative** for those who want to run VMs on-premises without the cloud bells and whistles. In our next post, we’ll dive into a feature-by-feature comparison to see how these alternatives stack up against VMware in functionality (spoiler: Hyper-V has a lot of enterprise capabilities that sometimes get overlooked!).

**References:**

1. Francesco Molfese – *"Windows Server 2025 vs. Azure Local: Who Wins the Virtualization Challenge?"* (Sep 2024) – **Hardware compatibility:** Windows Server offers greater flexibility if you want to reuse existing hardware.

2. Base-IT/Microsoft Webinar – *"Azure Local: The best infrastructure for hybrid and data modernization"* (2024) – Only reuse existing hardware for Azure Local if it **matches a current validated node**; otherwise new certified hardware is required.

3. Microsoft Learn – *"System requirements for Azure Local (22H2)"* (Nov 2023) – Azure Local **does not support external SAN storage or RAID controllers**; it requires direct-attached disks (Storage Spaces Direct) and identical server models in the cluster.

4. Microsoft Learn – *"Failover clustering hardware requirements and storage options"* (Nov 2024) – Windows Server Failover Clustering supports either **Storage Spaces Direct or traditional shared storage** (SAN/NAS). Clusters can use attached shared disks or SMB 3.0 shares for Hyper-V storage. It's **recommended (not required)** to use matching hardware for clusters, and all cluster hardware must pass validation tests for support.

5. Dell Azure Local Documentation – *"Hardware – Azure Local planning"* (Mar 2024) – Emphasizes following the **validated hardware path** for Azure Local to avoid issues; validated solutions are in the Azure Local Catalog and Microsoft **highly recommends** integrated systems.

6. BDRSuite Blog – *"Windows Server Hyper-V SAN vs. Storage Spaces Direct"* (Brandon Lee, 2017) – Notes that Storage Spaces Direct can often be **much cheaper than high-end SANs**, with storage management integrated into Windows Server (no separate SAN tools), making it an attractive option for new Hyper-V deployments.

7. Broadcom/VMware Knowledge Base – *"Deprecated devices in ESXi 9.0 and implications for support"* (KB#391170, 2025) – Warns that upgrading to VMware ESXi 9.0 with any **unsupported (EOL) devices** can result in loss of storage or network access; admins must **replace deprecated hardware before upgrading** to vSphere 9.0 to avoid non-supported configurations.

